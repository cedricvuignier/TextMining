
```{r setup, include=FALSE}
#ici les données sont avant et après, il y a toujours moyen de les filtrer selon les besoin
f <- file.path(here("data supervised_L/"), c("biden_after.csv","kamala_after.csv", "trump_after.csv", "vp_after.csv"))
d <- lapply(f, read.csv)
str(d, give.attr = FALSE)

d <- Map(cbind, d, name = c("Biden", "Kamala", "Trump", "VP"))

tweet_all <- rbind(d[[1]], d[[2]], d[[3]], d[[4]])
tweets.corpus <- corpus(tweet_all)


tweets.tokens <- tokens(tweets.corpus, 
                        remove_punct = TRUE, 
                        remove_symbols = TRUE, 
                        remove_url = TRUE,
                        what="word1")

tweets.tokens <- tokens_tolower(tweets.tokens) %>% tokens_wordstem() %>%
  tokens_remove(stopwords("english")) %>% 
  tokens_subset(ntoken(tweets.tokens) > 2) 

########## à partir de la ça doit être assez similaire aux exos du prof (supervised et unsupervised)
#j'ai pensé qu'on pourrait créer une variable y (partie politique -> prédire le partie)
#on peut aussi bien évidemmetn prédire celui qui à écrit le tweet ou d'autre variable 

names(tweet_all) #les diff variable du corpus
y <- factor(docvars(tweets.tokens, "screen_name")) #ici y = celui qui à écrit le tweet 

tweets.dfm <- dfm(tweets.tokens)
dim(tweets.dfm)
```


# Analysis GAETAN
```{r eval=FALSE, include=FALSE}


f <- file.path(here("data_before/"), c("BIDEN_all_tweet.csv","KAMALA_all_tweet.csv", "TRUMP_all_tweet.csv", "VP_all_tweet.csv"))
d <- lapply(f, read.csv)
str(d, give.attr = FALSE)

d <- Map(cbind, d, name = c("Biden", "Kamala", "Trump", "VP"))

test<- rbind(d[[1]], d[[2]], d[[3]], d[[4]])
test<- tibble(test)


test2 <- as.character(as.vector(test[,1])) 
tweets.corpus <- corpus(test2)

tweets.tokens <- tokens(tweets.corpus, 
                        remove_punct = TRUE, 
                        remove_symbols = TRUE, 
                        remove_url = TRUE,
                        what="word1") ## option for removing hastags and tags symbols (@, #, etc.)

tweets.tokens <- tokens_tolower(tweets.tokens) %>% tokens_wordstem() %>%
    tokens_remove(stopwords("english")) %>% 
    tokens_subset(ntoken(tweets.tokens) > 2) # removes tweets with 2 or fewer tokens


y <- factor(docvars(tweets.tokens, "handle"))

tweets.dfm <- dfm(tweets.tokens)
dim(tweets.dfm)

```