# sentiment Analysis
## before and after the election
```{r }
#data after
TRUMP_all_tweet <- read_csv("data supervised_L/trump_after.csv")
#get only the tweets after the election
Trump_after_analysis <- TRUMP_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date >= "2020-11-03") %>% 
  select(text)
#data before
TRUMP_all_tweet <- read_csv("data_before/TRUMP_all_tweet.csv")
#remove amp word" because I don't know why the sign & is transformed into "amp"
TRUMP_before_Analysis <- gsub("amp","", TRUMP_all_tweet)
TRUMP_before_Analysis <- as_tibble(TRUMP_all_tweet)
#
Trump_after_analysis <- gsub("amp","", Trump_after_analysis)
Trump_after_analysis <- as_tibble(Trump_after_analysis)

#transform in a tibble
before.trump <- as_tibble(data.frame(TRUMP_before_Analysis))
after.trump <- as_tibble(data.frame(Trump_after_analysis))
#tokenization

before.tok <- unnest_tokens(before.trump, output="word", input="value", to_lower=TRUE, strip_punct=TRUE, 
                           strip_numeric=TRUE)
after.tok <- unnest_tokens(after.trump, output="word", input="value", to_lower=TRUE, strip_punct=TRUE, 
                           strip_numeric=TRUE)
#remove stop words
before.tok <- anti_join(before.tok, stop_words, by = "word")
after.tok <- anti_join(after.tok, stop_words, by = "word")

#count the words use by trump 
before.fr <- before.tok %>%  count(word, sort=TRUE) %>% ungroup()
after.fr <- after.tok %>%  count(word, sort=TRUE) %>% ungroup() 

#wordcloud
before.fr <- before.fr 
wordcloud(words=before.fr$word, freq=before.fr$n, max.words=120, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
after.fr <- after.fr %>% 
  filter(n < 200)
wordcloud(words=after.fr$word, freq=after.fr$n, max.words=120, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
##########################################################################################
#sentimental analysis
before.sent.trump <- before.tok %>%
  inner_join(get_sentiments("nrc"))

after.sent.trump <- after.tok %>%
  inner_join(get_sentiments("nrc"))

#before
before.sent.trump %>% group_by(sentiment) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(x = reorder(sentiment, -n) , y = n, fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) +
  ggtitle("Sentiment Analysis of Donald Trump's tweets") +
  xlab("sentiment") + ylab("nb") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
#after
after.sent.trump %>% group_by(sentiment) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(x = reorder(sentiment, -n) , y = n, fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) +
  ggtitle("Sentiment Analysis of Donald Trump's tweets") +
  xlab("sentiment") + ylab("nb") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))


```

## after the election

```{r echo=FALSE, message=FALSE, warning=FALSE, , fig.height= 4, fig.height=4}
#republicans
# trump analysis
TRUMP_all_tweet <- read_csv("data supervised_L/trump_after.csv")
#get only the tweets after the election
Trump_after_analysis <- TRUMP_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date >= "2020-11-03")
#get the tweet before
Trump_before_analysis <- TRUMP_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date < "2020-11-03")
#add variable before or after
Trump_after_analysis <-Trump_after_analysis %>% 
  mutate(when = "after")

Trump_before_analysis <-Trump_before_analysis %>% 
  mutate(when = "before")
#############
Trump_after_analysis <- Trump_after_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(Trump_after_analysis$text, collapse = '')) 

Trump_after_analysis <- rename(Trump_after_analysis, text = `paste(Trump_after_analysis$text, collapse = "")`)
  
Trump_before_analysis <- Trump_before_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(Trump_before_analysis$text, collapse = '')) 

Trump_before_analysis <- rename(Trump_before_analysis, text = `paste(Trump_before_analysis$text, collapse = "")`)
###################### DEMOCRATE
# biden analysis
BIDEN_all_tweet <- read_csv("data supervised_L/biden_after.csv")
#get only the tweets after the election
Biden_after_analysis <- BIDEN_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date >= "2020-11-03")
#get the tweet before
Biden_before_analysis <- BIDEN_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date < "2020-11-03")
#add variable before or after
Biden_after_analysis <-Biden_after_analysis %>% 
  mutate(when = "after")

Biden_before_analysis <-Biden_before_analysis %>% 
  mutate(when = "before")
#############
Biden_after_analysis <- Biden_after_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(Biden_after_analysis$text, collapse = '')) 

Biden_after_analysis <- rename(Biden_after_analysis, text = `paste(Biden_after_analysis$text, collapse = "")`)
  
Biden_before_analysis <- Biden_before_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(Biden_before_analysis$text, collapse = '')) 

Biden_before_analysis <- rename(Biden_before_analysis, text = `paste(Biden_before_analysis$text, collapse = "")`)

  
Tweet <- rbind(Trump_before_analysis,Trump_after_analysis, Biden_before_analysis, Biden_after_analysis)

Tweet_corpus <- corpus(Tweet, 
                     docvars = data.frame(party = names(Tweet)))
#####################################
## with quanteda
tweet.tok <- tokens(Tweet_corpus,
                     remove_punct = TRUE,
                     remove_symbols = TRUE,
                     remove_numbers = TRUE)
tweet.tok <- tokens_tolower(tweet.tok)
tweet.tok <- tokens_replace(tweet.tok, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma)
tweet.tok <- tokens_remove(tweet.tok, pattern = stopwords("english")) 
## Note: lemmatization may change some words to stopwords (like "yeah" to "yes"). 
## Stopwords cleaning should be performed at the end

## Make a sentiment analysis 
tweet.sent <- tokens_lookup(tweet.tok, dictionary = data_dictionary_LSD2015) %>% dfm()
tweet.sent.td <- tidy(tweet.sent)
ggplot(tweet.sent.td,aes(x=document, y=count, fill=term)) + geom_bar(stat="identity") + coord_flip()

## with tidytext
tweet.tb <- tidy(Tweet_corpus) %>% mutate(Document=paste(when,"_",screen_name,sep=""))
tweet.tok <- unnest_tokens(tweet.tb, output="word", input="text", to_lower=TRUE, strip_punct=TRUE, 
                           strip_numeric=TRUE)
tweet.sent <- tweet.tok %>%
  inner_join(get_sentiments("nrc"))


table(tweet.sent$Document, tweet.sent$sentiment)

tweet.sent %>% group_by(Document, sentiment) %>% summarize(n = n()) %>% 
  ggplot(aes(x = sentiment, y = n, fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ Document) + coord_flip()

## Same with sentiment frequencies per document
tweet.sent %>% group_by(Document, sentiment) %>%  summarize(n = n()) %>% 
  mutate(freq = n / sum(n)) %>% ggplot(aes(x = sentiment, y = freq, fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ Document) + coord_flip()

## With afinn
tweet.sent <- tweet.tok %>%
  inner_join(get_sentiments("afinn"))
tweet.sent
## Summarize per document (value average) + barplot
aggregate(value~Document, data=tweet.sent, FUN=mean)

aggregate(value~Document, data=tweet.sent, FUN=mean) %>% 
  ggplot(aes(x = Document, y = value)) + 
  geom_bar(stat = "identity") + coord_flip()

library(sentimentr)
library(lexicon)
## Valence shifter approach
crude.tweet <- get_sentences(Tweet_corpus[1:2])
crude.senti <- sentiment(crude.tweet) 
crude.senti <- as_tibble(crude.senti)

library(splines)
crude.senti %>% group_by(element_id) %>% 
  ggplot(aes(x = sentence_id, y = sentiment)) + 
  geom_line() +  
  geom_smooth(se=FALSE, formula=y ~ bs(x), method="lm") +
  facet_wrap(~ element_id, scale="free_x")


```



```{r}
library(readr)
library(quanteda)
library(tidyr)
#merge all the tweet into one document 
#biden
Biden_crude <- read_delim("data_before/BIDEN_all_tweet.csv", "\t", escape_double = FALSE, trim_ws = TRUE)

Biden_doc <- paste(Biden_crude$value, collapse = '') %>% 
  as.data.frame() 
Biden_doc <- rename(Biden_doc, Text = .)
#kamala
Kamala_crude <- read_delim("data_before/KAMALA_all_tweet.csv", "\t", escape_double = FALSE, trim_ws = TRUE)

Kamala_doc <- paste(Kamala_crude$value, collapse = '') %>% 
  as.data.frame() 
Kamala_doc <- rename(Kamala_doc, Text = .)
#TRUMP
Trump_crude <- read_delim("data_before/TRUMP_all_tweet.csv", "\t", escape_double = FALSE, trim_ws = TRUE)

Trump_doc <- paste(Trump_crude$value, collapse = '') %>% 
  as.data.frame() 
Trump_doc <- rename(Trump_doc, Text = .)
#VP
vp_crude <- read_delim("data_before/VP_all_tweet.csv", "\t", escape_double = FALSE, trim_ws = TRUE)

Vp_doc <- paste(vp_crude$value, collapse = '') %>% 
  as.data.frame() 
Vp_doc <- rename(Vp_doc, Text = .)
#hashtag trump
hashtag_Trump_crude <- read_delim("data_before/hashtag_trump.csv", "\t", escape_double = FALSE, trim_ws = TRUE)
H_trump_doc <- paste(hashtag_Trump_crude$value, collapse = '') %>% 
  as.data.frame() 
H_trump_doc <- rename(H_trump_doc, Text = .)
#hashtag biden
hashtag_Biden_crude <- read_delim("data_before/hashtag_biden.csv", "\t", escape_double = FALSE, trim_ws = TRUE)
H_biden_doc <- paste(hashtag_Biden_crude$value, collapse = '') %>% 
  as.data.frame() 
H_biden_doc <- rename(H_biden_doc, Text = .)

#merge all the doc
Before_Tweet <- as.data.frame(c(Biden_doc,Kamala_doc,Trump_doc,Vp_doc,H_trump_doc,H_biden_doc))
#pivot
candidat <- c("Biden_doc", "Kamala_doc", "Trump_doc", "Vp_doc" , "H_trump_doc", "H_biden_doc")
party <- c("democrate", "democrate", "republican", "republican" , "people", "people")

Before_Tweet <- Before_Tweet %>% 
   pivot_longer(
     cols = starts_with("Text"))

Before_Tweet <- mutate(Before_Tweet, candidat = candidat)
Before_Tweet <- mutate(Before_Tweet, party = party)
#create a corpus
before_corpus <- corpus(Before_Tweet,text_field = "value")

#rm(list = setdiff(ls(), c("before_corpus", "Before_Tweet")))
```

# EDA analysis

```{r message=FALSE, warning=FALSE}
before.tok <- tokens(before_corpus,
                     remove_punct = TRUE,
                     remove_symbols = TRUE,
                     remove_numbers = TRUE)
library(lexicon) # needed for hash_lemmas used below for the lemmatization
before.tok <- tokens_replace(before.tok, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma)
before.tok <- tokens_remove(before.tok, pattern = c("s", "amp", "de","la", "en", stopwords("english"))) 

## create the dfm
before.dfm <- dfm(before.tok)
# 15 most frequents terms
textstat_frequency(before.dfm, n = 10) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  labs(x = "Term", y = "Frequency") + coord_flip()

## create the tf-idf; represent the 20 largest ones
tweet.tok2 <- tokens(corpus_subset(before_corpus),
                      remove_punct = TRUE,
                      remove_symbols = TRUE,
                      remove_numbers = TRUE) %>%  
  tokens_replace(pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) %>% 
  tokens_remove(pattern = c("s", "amp", "de","la", "en", "t", "gt", "e", "hashta", stopwords("english")))
  

tweet.tfidf <- dfm(tweet.tok2) %>% dfm_tfidf() %>% tidy()

tweet.tfidf %>%
  top_n(20) %>% 
  ggplot(aes(term, count, fill = document)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") + coord_flip() + 
  facet_wrap(~document, ncol = 2)

## Make a cloud of words (frequency based below, only terms with frequency >= 10)
set.seed(11)
tweet.tok2 %>% dfm() %>% dfm_trim(min_termfreq = 10) %>% textplot_wordcloud()

## Compare the speeches in terms of lexical diversity
textstat_lexdiv(before.tok, measure = "I") %>% 
  ggplot(aes(x=reorder(document, I), y=I))+geom_point()+coord_flip()+
  xlab("Text") + ylab("Yule's index")

## Compare the tweets of republican vs to the tweets of democrat in terms of keyness
tweet.tok3 <- tokens(corpus_subset(before_corpus, party %in% c("democrate", "republican")),
                      remove_punct = TRUE,
                      remove_symbols = TRUE,
                      remove_numbers = TRUE) %>%
  tokens_replace(pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) %>%
  tokens_remove(pattern = c("s", "amp", "de","la", "en", "t", stopwords("english")))
tweet.tok3 %>% dfm(groups="party") %>% textstat_keyness(target="democrate") %>% textplot_keyness()

## Compare the tweets of Trump vs to the tweets of Biden in terms of keyness
tweet.tok4 <- tokens(corpus_subset(before_corpus, candidat %in% c("Trump_doc", "Biden_doc")),
                      remove_punct = TRUE,
                      remove_symbols = TRUE,
                      remove_numbers = TRUE) %>%
  tokens_replace(pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) %>%
  tokens_remove(pattern = c("s", "amp", "de","la", "en", "t", stopwords("english")))
tweet.tok4 %>% dfm(groups="candidat") %>% textstat_keyness(target="Biden_doc") %>% textplot_keyness()

## Compare the tweets using hashtag Trump vs to the tweets using hashtag Biden in terms of keyness
tweet.tok5 <- tokens(corpus_subset(before_corpus, candidat %in% c("H_trump_doc", "H_biden_doc")),
                      remove_punct = TRUE,
                      remove_symbols = TRUE,
                      remove_numbers = TRUE) %>%
  tokens_replace(pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) %>%
  tokens_remove(pattern = c("s", "amp", "de","la", "en", "t", "gt", "e", "hashta", stopwords("english")))
tweet.tok5 %>% dfm(groups="candidat") %>% textstat_keyness(target="H_biden_doc") %>% textplot_keyness()
```


```{r}
#delete after
# retrieving docvars from a corpus
head(docvars(data_corpus_inaugural))
tail(docvars(data_corpus_inaugural, "President"), 10)
head(data_corpus_inaugural$President)
head(before_corpus$party)

test <- as.data.frame(data_corpus_inaugural$Party)
# assigning document variables to a corpus
corp <- data_corpus_inaugural
docvars(corp, "President") <- paste("prez", 1:ndoc(corp), sep = "")
head(docvars(corp))
corp$fullname <- paste(data_corpus_inaugural$FirstName,
                       data_corpus_inaugural$President)
tail(corp$fullname)



# accessing or assigning docvars for a corpus using "$"
data_corpus_inaugural$Year
data_corpus_inaugural$century <- floor(data_corpus_inaugural$Year / 100)
data_corpus_inaugural$century

# accessing or assigning docvars for tokens using "$"
toks <- tokens(corpus_subset(data_corpus_inaugural, Year <= 1805))
toks$Year
toks$Year <- 1991:1995
toks$Year
toks$nonexistent <- TRUE
docvars(toks)

# accessing or assigning docvars for a dfm using "$"
dfmat <- dfm(toks)
dfmat$Year
dfmat$Year <- 1991:1995
dfmat$Year
dfmat$nonexistent <- TRUE
docvars(dfmat)
```


```{r }

#Type-token Ratio
tweet.dfm <- dfm(before_corpus,
                   stem=FALSE,
                   tolower=TRUE,
                   remove=c("reuter",stopwords("english")),
                   remove_punct=TRUE,
                   remove_number=TRUE,
                   remove_symbols=TRUE)
tweet.div <- textstat_lexdiv(tweet.dfm, measure = "I")
tweet.div %>% 
  ggplot(aes(x=reorder(document, I), y=I))+geom_point()+coord_flip()+
  xlab("Text") + ylab("Yule's index")
```

```{r}
#MATTR
tweet.tok <- tokens(before_corpus,
                    remove_punct=TRUE,
                    remove_number=TRUE,
                    remove_symbols = TRUE)
tweet.tok <- tokens_remove(tweet.tok, stopwords("english"))
tweet.tok <- tokens_tolower(tweet.tok)
tweet.div <- textstat_lexdiv(tweet.tok, measure = "MATTR", MATTR_window = 10)
tweet.div %>% 
  ggplot(aes(x=reorder(document, MATTR), y=MATTR))+geom_point()+coord_flip()+
  xlab("Text") + ylab("MATTR")
```

