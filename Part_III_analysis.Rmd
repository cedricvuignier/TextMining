# sentiment Analysis
## before the election

```{r echo=FALSE, message=FALSE, warning=FALSE, , fig.height= 4, fig.height=4}
#republicans
# trump analysis
TRUMP_all_tweet <- read_csv("data_before/TRUMP_all_tweet.csv")
VP_all_tweet <- read_csv("data_before/VP_all_tweet.csv")

#remove amp word" because I don't know why the sign & is transformed into "amp"
TRUMP_all_tweet <- gsub("amp","", TRUMP_all_tweet)
TRUMP_all_tweet <- as_tibble(TRUMP_all_tweet)
#
VP_all_tweet <- gsub("amp","", VP_all_tweet)
VP_all_tweet <- as_tibble(VP_all_tweet)

#transform in a tibble
crude.trump <- as_tibble(data.frame(TRUMP_all_tweet))
#
crude.vp <- as_tibble(data.frame(VP_all_tweet))

#tokenization
crude.tok <- unnest_tokens(crude.trump, output="word", input="value", to_lower=TRUE, strip_punct=TRUE, 
                           strip_numeric=TRUE)
#
crude.tok.vp <- unnest_tokens(crude.vp, output="word", input="value", to_lower=TRUE, strip_punct=TRUE, 
                           strip_numeric=TRUE)
#remove stop words
crude.tok <- anti_join(crude.tok, stop_words, by = "word")
#
crude.tok.vp <- anti_join(crude.tok.vp, stop_words, by = "word")

#count the words use by trump 
crude.fr <- crude.tok %>%  count(word, sort=TRUE) %>% ungroup()
#
crude.fr.vp <- crude.tok.vp %>%  count(word, sort=TRUE) %>% ungroup()

#wordcloud
crude.fr <- crude.fr 
wordcloud(words=crude.fr$word, freq=crude.fr$n, max.words=120, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
#
crude.fr.vp <- crude.fr.vp 
wordcloud(words=crude.fr.vp$word, freq=crude.fr.vp$n, max.words=120, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
##########################################################################################
#sentimental analysis
crude.sent.trump <- crude.tok %>%
  inner_join(get_sentiments("nrc"))

crude.sent %>% group_by(sentiment) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(x = reorder(sentiment, -n) , y = n, fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) +
  ggtitle("Sentiment Analysis of Donald Trump's tweets") +
  xlab("sentiment") + ylab("nb") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
#
crude.sent.vp <- crude.tok.vp %>%
  inner_join(get_sentiments("nrc"))

crude.sent.vp %>% group_by(sentiment) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(x = reorder(sentiment, -n) , y = n, fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) +
  ggtitle("Sentiment Analysis of Mike Pence's tweets") +
  xlab("sentiment") + ylab("nb") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))

```


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4, fig.height=4}
#democrate
#biden and kamala
BIDEN_all_tweet <- read_csv("data_before/BIDEN_all_tweet.csv")
kamala_all_tweet <- read_csv("data_before/KAMALA_all_tweet.csv")
#remove amp word" because I don't know why the sign & is transformed into "amp"
BIDEN_all_tweet <- gsub("amp","", BIDEN_all_tweet)
BIDEN_all_tweet <- as_tibble(BIDEN_all_tweet)
#
kamala_all_tweet <- gsub("amp","", kamala_all_tweet)
kamala_all_tweet <- as_tibble(kamala_all_tweet)

#transform in a tibble
crude.biden <- as_tibble(data.frame(BIDEN_all_tweet))
#
crude.kamala <- as_tibble(data.frame(kamala_all_tweet))

#tokenization
crude.tok <- unnest_tokens(crude.biden, output="word", input="value", to_lower=TRUE, strip_punct=TRUE, 
                           strip_numeric=TRUE)
#
crude.tok.kamala <- unnest_tokens(crude.kamala, output="word", input="value", to_lower=TRUE, strip_punct=TRUE, 
                           strip_numeric=TRUE)
#remove stop words
crude.tok <- anti_join(crude.tok, stop_words, by = "word")
#
crude.tok.kamala <- anti_join(crude.tok.kamala, stop_words, by = "word")

#count the words use by biden and kamala 
crude.fr <- crude.tok %>%  count(word, sort=TRUE) %>% ungroup()
#
crude.fr.kamala <- crude.tok.kamala %>%  count(word, sort=TRUE) %>% ungroup()

#wordcloud
crude.fr <- crude.fr 
wordcloud(words=crude.fr$word, freq=crude.fr$n, max.words=120, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
#
crude.fr.kamala <- crude.fr.kamala 
wordcloud(words=crude.fr.kamala$word, freq=crude.fr.kamala$n, max.words=120, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

##########################################################################################
#sentimental analysis

crude.sent <- crude.tok %>%
  inner_join(get_sentiments("nrc"))

crude.sent %>% group_by(sentiment) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(x = reorder(sentiment, -n) , y = n, fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) +
  ggtitle("Sentiment Analysis of Joe Biden's tweets") +
  xlab("sentiment") + ylab("nb") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
#
crude.sent.kamala <- crude.tok.kamala %>%
  inner_join(get_sentiments("nrc"))

crude.sent.kamala %>% group_by(sentiment) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(x = reorder(sentiment, -n) , y = n, fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) +
  ggtitle("Sentiment Analysis of Joe Biden's tweets") +
  xlab("sentiment") + ylab("nb") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
```
## after the election

```{r echo=FALSE, message=FALSE, warning=FALSE, , fig.height= 4, fig.height=4}
#republicans
# trump analysis
TRUMP_all_tweet <- read_csv("data_before/TRUMP_all_tweet.csv")
VP_all_tweet <- read_csv("data_before/VP_all_tweet.csv")

#remove amp word" because I don't know why the sign & is transformed into "amp"
TRUMP_all_tweet <- gsub("amp","", TRUMP_all_tweet)
TRUMP_all_tweet <- as_tibble(TRUMP_all_tweet)
#
VP_all_tweet <- gsub("amp","", VP_all_tweet)
VP_all_tweet <- as_tibble(VP_all_tweet)

#transform in a tibble
crude.trump <- as_tibble(data.frame(TRUMP_all_tweet))
#
crude.vp <- as_tibble(data.frame(VP_all_tweet))

#tokenization
crude.tok <- unnest_tokens(crude.trump, output="word", input="value", to_lower=TRUE, strip_punct=TRUE, 
                           strip_numeric=TRUE)
#
crude.tok.vp <- unnest_tokens(crude.vp, output="word", input="value", to_lower=TRUE, strip_punct=TRUE, 
                           strip_numeric=TRUE)
#remove stop words
crude.tok <- anti_join(crude.tok, stop_words, by = "word")
#
crude.tok.vp <- anti_join(crude.tok.vp, stop_words, by = "word")

#count the words use by trump 
crude.fr <- crude.tok %>%  count(word, sort=TRUE) %>% ungroup()
#
crude.fr.vp <- crude.tok.vp %>%  count(word, sort=TRUE) %>% ungroup()

#wordcloud
crude.fr <- crude.fr 
wordcloud(words=crude.fr$word, freq=crude.fr$n, max.words=120, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
#
crude.fr.vp <- crude.fr.vp 
wordcloud(words=crude.fr.vp$word, freq=crude.fr.vp$n, max.words=120, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
##########################################################################################
#sentimental analysis
crude.sent.trump <- crude.tok %>%
  inner_join(get_sentiments("nrc"))

crude.sent %>% group_by(sentiment) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(x = reorder(sentiment, -n) , y = n, fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) +
  ggtitle("Sentiment Analysis of Donald Trump's tweets") +
  xlab("sentiment") + ylab("nb") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
#
crude.sent.vp <- crude.tok.vp %>%
  inner_join(get_sentiments("nrc"))

crude.sent.vp %>% group_by(sentiment) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(x = reorder(sentiment, -n) , y = n, fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) +
  ggtitle("Sentiment Analysis of Mike Pence's tweets") +
  xlab("sentiment") + ylab("nb") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))

```

# Analysis GAETAN
## j'ai mis ton code en option qu'il ne run pas juste pour knit 
```{r eval=FALSE, include=FALSE}


f <- file.path(here("data_before/"), c("BIDEN_all_tweet.csv","KAMALA_all_tweet.csv", "TRUMP_all_tweet.csv", "VP_all_tweet.csv"))
d <- lapply(f, read.csv)
str(d, give.attr = FALSE)

d <- Map(cbind, d, name = c("Biden", "Kamala", "Trump", "VP"))

test<- rbind(d[[1]], d[[2]], d[[3]], d[[4]])
test<- tibble(test)


test2 <- as.character(as.vector(test[,1])) 
tweets.corpus <- corpus(test2)

tweets.tokens <- tokens(tweets.corpus, 
                        remove_punct = TRUE, 
                        remove_symbols = TRUE, 
                        remove_url = TRUE,
                        what="word1") ## option for removing hastags and tags symbols (@, #, etc.)

tweets.tokens <- tokens_tolower(tweets.tokens) %>% tokens_wordstem() %>%
    tokens_remove(stopwords("english")) %>% 
    tokens_subset(ntoken(tweets.tokens) > 2) # removes tweets with 2 or fewer tokens


y <- factor(docvars(tweets.tokens, "handle"))

tweets.dfm <- dfm(tweets.tokens)
dim(tweets.dfm)

```

```{r}
library(readr)
library(quanteda)
library(tidyr)
#merge all the tweet into one document 
#biden
Biden_crude <- read_delim("data_before/BIDEN_all_tweet.csv", "\t", escape_double = FALSE, trim_ws = TRUE)

Biden_doc <- paste(Biden_crude$value, collapse = '') %>% 
  as.data.frame() 
Biden_doc <- rename(Biden_doc, Text = .)
#kamala
Kamala_crude <- read_delim("data_before/KAMALA_all_tweet.csv", "\t", escape_double = FALSE, trim_ws = TRUE)

Kamala_doc <- paste(Kamala_crude$value, collapse = '') %>% 
  as.data.frame() 
Kamala_doc <- rename(Kamala_doc, Text = .)
#TRUMP
Trump_crude <- read_delim("data_before/TRUMP_all_tweet.csv", "\t", escape_double = FALSE, trim_ws = TRUE)

Trump_doc <- paste(Trump_crude$value, collapse = '') %>% 
  as.data.frame() 
Trump_doc <- rename(Trump_doc, Text = .)
#VP
vp_crude <- read_delim("data_before/VP_all_tweet.csv", "\t", escape_double = FALSE, trim_ws = TRUE)

Vp_doc <- paste(vp_crude$value, collapse = '') %>% 
  as.data.frame() 
Vp_doc <- rename(Vp_doc, Text = .)
#hashtag trump
hashtag_Trump_crude <- read_delim("data_before/hashtag_trump.csv", "\t", escape_double = FALSE, trim_ws = TRUE)
H_trump_doc <- paste(hashtag_Trump_crude$value, collapse = '') %>% 
  as.data.frame() 
H_trump_doc <- rename(H_trump_doc, Text = .)
#hashtag biden
hashtag_Biden_crude <- read_delim("data_before/hashtag_biden.csv", "\t", escape_double = FALSE, trim_ws = TRUE)
H_biden_doc <- paste(hashtag_Biden_crude$value, collapse = '') %>% 
  as.data.frame() 
H_biden_doc <- rename(H_biden_doc, Text = .)

#merge all the doc
Before_Tweet <- as.data.frame(c(Biden_doc,Kamala_doc,Trump_doc,Vp_doc,H_trump_doc,H_biden_doc))
#pivot
candidat <- c("Biden_doc", "Kamala_doc", "Trump_doc", "Vp_doc" , "H_trump_doc", "H_biden_doc")
party <- c("democrate", "democrate", "republican", "republican" , "people", "people")

Before_Tweet <- Before_Tweet %>% 
   pivot_longer(
     cols = starts_with("Text"))

Before_Tweet <- mutate(Before_Tweet, candidat = candidat)
Before_Tweet <- mutate(Before_Tweet, party = party)
#create a corpus
before_corpus <- corpus(Before_Tweet,text_field = "value")

#rm(list = setdiff(ls(), c("before_corpus", "Before_Tweet")))
```

# EDA analysis

```{r message=FALSE, warning=FALSE}
before.tok <- tokens(before_corpus,
                     remove_punct = TRUE,
                     remove_symbols = TRUE,
                     remove_numbers = TRUE)
library(lexicon) # needed for hash_lemmas used below for the lemmatization
before.tok <- tokens_replace(before.tok, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma)
before.tok <- tokens_remove(before.tok, pattern = c("s", "amp", "de","la", "en", stopwords("english"))) 

## create the dfm
before.dfm <- dfm(before.tok)
# 15 most frequents terms
textstat_frequency(before.dfm, n = 10) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  labs(x = "Term", y = "Frequency") + coord_flip()

## create the tf-idf; represent the 20 largest ones
tweet.tok2 <- tokens(corpus_subset(before_corpus),
                      remove_punct = TRUE,
                      remove_symbols = TRUE,
                      remove_numbers = TRUE) %>%  
  tokens_replace(pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) %>% 
  tokens_remove(pattern = c("s", "amp", "de","la", "en", "t", "gt", "e", "hashta", stopwords("english")))
  

tweet.tfidf <- dfm(tweet.tok2) %>% dfm_tfidf() %>% tidy()

tweet.tfidf %>%
  top_n(20) %>% 
  ggplot(aes(term, count, fill = document)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") + coord_flip() + 
  facet_wrap(~document, ncol = 2)

## Make a cloud of words (frequency based below, only terms with frequency >= 10)
set.seed(11)
tweet.tok2 %>% dfm() %>% dfm_trim(min_termfreq = 10) %>% textplot_wordcloud()

## Compare the speeches in terms of lexical diversity
textstat_lexdiv(before.tok, measure = "I") %>% 
  ggplot(aes(x=reorder(document, I), y=I))+geom_point()+coord_flip()+
  xlab("Text") + ylab("Yule's index")

## Compare the tweets of republican vs to the tweets of democrat in terms of keyness
tweet.tok3 <- tokens(corpus_subset(before_corpus, party %in% c("democrate", "republican")),
                      remove_punct = TRUE,
                      remove_symbols = TRUE,
                      remove_numbers = TRUE) %>%
  tokens_replace(pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) %>%
  tokens_remove(pattern = c("s", "amp", "de","la", "en", "t", stopwords("english")))
tweet.tok3 %>% dfm(groups="party") %>% textstat_keyness(target="democrate") %>% textplot_keyness()

## Compare the tweets of Trump vs to the tweets of Biden in terms of keyness
tweet.tok4 <- tokens(corpus_subset(before_corpus, candidat %in% c("Trump_doc", "Biden_doc")),
                      remove_punct = TRUE,
                      remove_symbols = TRUE,
                      remove_numbers = TRUE) %>%
  tokens_replace(pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) %>%
  tokens_remove(pattern = c("s", "amp", "de","la", "en", "t", stopwords("english")))
tweet.tok4 %>% dfm(groups="candidat") %>% textstat_keyness(target="Biden_doc") %>% textplot_keyness()

## Compare the tweets using hashtag Trump vs to the tweets using hashtag Biden in terms of keyness
tweet.tok5 <- tokens(corpus_subset(before_corpus, candidat %in% c("H_trump_doc", "H_biden_doc")),
                      remove_punct = TRUE,
                      remove_symbols = TRUE,
                      remove_numbers = TRUE) %>%
  tokens_replace(pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) %>%
  tokens_remove(pattern = c("s", "amp", "de","la", "en", "t", "gt", "e", "hashta", stopwords("english")))
tweet.tok5 %>% dfm(groups="candidat") %>% textstat_keyness(target="H_biden_doc") %>% textplot_keyness()
```


```{r}
#delete after
# retrieving docvars from a corpus
head(docvars(data_corpus_inaugural))
tail(docvars(data_corpus_inaugural, "President"), 10)
head(data_corpus_inaugural$President)
head(before_corpus$party)

test <- as.data.frame(data_corpus_inaugural$Party)
# assigning document variables to a corpus
corp <- data_corpus_inaugural
docvars(corp, "President") <- paste("prez", 1:ndoc(corp), sep = "")
head(docvars(corp))
corp$fullname <- paste(data_corpus_inaugural$FirstName,
                       data_corpus_inaugural$President)
tail(corp$fullname)



# accessing or assigning docvars for a corpus using "$"
data_corpus_inaugural$Year
data_corpus_inaugural$century <- floor(data_corpus_inaugural$Year / 100)
data_corpus_inaugural$century

# accessing or assigning docvars for tokens using "$"
toks <- tokens(corpus_subset(data_corpus_inaugural, Year <= 1805))
toks$Year
toks$Year <- 1991:1995
toks$Year
toks$nonexistent <- TRUE
docvars(toks)

# accessing or assigning docvars for a dfm using "$"
dfmat <- dfm(toks)
dfmat$Year
dfmat$Year <- 1991:1995
dfmat$Year
dfmat$nonexistent <- TRUE
docvars(dfmat)
```


```{r }

#Type-token Ratio
tweet.dfm <- dfm(before_corpus,
                   stem=FALSE,
                   tolower=TRUE,
                   remove=c("reuter",stopwords("english")),
                   remove_punct=TRUE,
                   remove_number=TRUE,
                   remove_symbols=TRUE)
tweet.div <- textstat_lexdiv(tweet.dfm, measure = "I")
tweet.div %>% 
  ggplot(aes(x=reorder(document, I), y=I))+geom_point()+coord_flip()+
  xlab("Text") + ylab("Yule's index")
```

```{r}
#MATTR
tweet.tok <- tokens(before_corpus,
                    remove_punct=TRUE,
                    remove_number=TRUE,
                    remove_symbols = TRUE)
tweet.tok <- tokens_remove(tweet.tok, stopwords("english"))
tweet.tok <- tokens_tolower(tweet.tok)
tweet.div <- textstat_lexdiv(tweet.tok, measure = "MATTR", MATTR_window = 10)
tweet.div %>% 
  ggplot(aes(x=reorder(document, MATTR), y=MATTR))+geom_point()+coord_flip()+
  xlab("Text") + ylab("MATTR")
```

