---
title: "Text Mining- Group D"
author: "Cédric Vuignier, Gaëtan Lovey"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    code_folding: hide
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	eval = FALSE
)
```

```{r include=FALSE}
library(graphics)
library(textclean)
library(stringr)
library(purrr)
library(rlang)
library(dplyr)
library(tibble)
library(syuzhet)
library(NLP)
library(tm)
library(rtweet)
library(reactable)
library(httpuv)
library(here)
library(readtext)
library(foreign)
library(quanteda)
library(readr)
library(lubridate)
library(ggplot2)
library(kableExtra)
library(tidyverse)
library(tidytext)
library(textdata)
library(wordcloud)
library(quanteda.textmodels)
library(caret)
library(ranger)
library(lexicon)
library(qdapRegex)
library(text2vec)
```

# Introduction

## Useful litterature

- Clean the tweets : https://towardsdatascience.com/text-mining-with-r-gathering-and-cleaning-data-8f8b0d65e67c 
- Set the twitter API : https://www.infoworld.com/article/3515712/how-to-search-twitter-with-rtweet-and-r.html  
- Scrap by hashtag, region or user : https://cran.r-project.org/web/packages/rtweet/vignettes/intro.html 

## Connect to Twitter API

Twitter log-in (need to register the account in the pop-up windows)
name : textmining13
mdp : textmining2020

```{r message=FALSE, warning=FALSE, eval=FALSE}
library(rtweet)
get_token()
```

We do not need to run the following code if has been already done. 
 
Follow the instructions to link your Twitter app with your R console. Do it only once. 
```{r message=FALSE, warning=FALSE, eval=FALSE}
#load rtweet
library(rtweet)

#store api keys 
api_key <- "cFRZi4lsJGD4DxrtLT24za5UH"
api_secret_key <- "iHBlU1fA5WdQ17N3udi8FWg8TlwugXb8e6c34MNRGmKXNJFIQY"

#authenticate via web browser
token <- create_token(
  app = "TextMiningHEC",
  consumer_key = api_key,
  consumer_secret = api_secret_key)
```

## Tweets scraping

### Get the tweets

We collect the 3000 last tweets from Trump, Joe Biden, Kamala Harris, Mike Pence and Alexandria Ocasio-Cortez. The data frame has 90 variables.

Twitter sets a limit of 3200 tweets per request. So we have to do the same operation for each account (@).

```{r message=FALSE, warning=FALSE, eval=FALSE}
#split the scrapping into two part to avoid the limit 

#republican
tweet_trump <- get_timelines("realDonaldTrump", n = 3000, include_rts = FALSE)

#vice president tweets
tweet_VP <- get_timelines("VP", n = 3000, include_rts = TRUE)


#get the Trump hashtag
tweet_trump_hashtag <- search_tweets(
  "#Trump", n = 3000, include_rts = FALSE
)

#democrate
tweet_BIDEN <- get_timelines("JoeBiden", n = 3000, 
            include_rts = FALSE)

#get the Biden hashtag
tweet_biden_hashtag <- search_tweets(
  "#Biden", n = 3000, include_rts = FALSE
)

#the vice president selected by Biden if he is elected 
tweet_KAMALA <- get_timelines("KamalaHarris", n = 3000, 
            include_rts = FALSE)

#Obama
tweet_OBAMA <- get_timelines("BarackObama", n = 3000, 
            include_rts = FALSE)       

#Alexandria Ocasio-Cortez
tweet_ACO <- get_timelines("AOC", n = 3000, 
            include_rts = FALSE)    
```

## Creation of the corpus

We created two corpus: 

- Tweets from before / after the election day
- All the tweets

### Corpus of before and after the election day

```{r}
#trump analysis
TRUMP_all_tweet <- read_csv("data/trump_after.csv")

#get only the tweets after the election
Trump_after_analysis <- TRUMP_all_tweet %>% 
  mutate(date = date(created_at)) %>% filter(date >= "2020-11-03", 
                                             display_text_width > 100)
#get the tweets before the election
Trump_before_analysis <- TRUMP_all_tweet %>% 
  mutate(date = date(created_at)) %>% filter(date < "2020-11-03", 
                                             display_text_width > 100)
#add variable before or after
Trump_after_analysis <-Trump_after_analysis %>% 
  mutate(when = "after")
Trump_before_analysis <-Trump_before_analysis %>% 
  mutate(when = "before")

#book of tweets "after" 
Trump_after_analysis <- Trump_after_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(Trump_after_analysis$text, collapse = '. ')) 

Trump_after_analysis <- rename(Trump_after_analysis, text = `paste(Trump_after_analysis$text, collapse = ". ")`)

#book of tweets "before"  
Trump_before_analysis <- Trump_before_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(Trump_before_analysis$text, collapse = '. ')) 

Trump_before_analysis <- rename(Trump_before_analysis, text = `paste(Trump_before_analysis$text, collapse = ". ")`)

#vice president analysis
vp_all_tweet <- read_csv("data/vp_after.csv")

#get only the tweets after the election
vp_after_analysis <- vp_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date >= "2020-11-03",
        display_text_width > 100)

#get the tweets before the election
vp_before_analysis <- vp_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date < "2020-11-03",
        display_text_width > 100)

#add variable before or after
vp_after_analysis <-vp_after_analysis %>% 
  mutate(when = "after")

vp_before_analysis <-vp_before_analysis %>% 
  mutate(when = "before")

vp_after_analysis <- vp_after_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(vp_after_analysis$text, collapse = '. ')) 

vp_after_analysis <- rename(vp_after_analysis, text = `paste(vp_after_analysis$text, collapse = ". ")`)
  
vp_before_analysis <- vp_before_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(vp_before_analysis$text, collapse = '. ')) 

vp_before_analysis <- rename(vp_before_analysis, text = `paste(vp_before_analysis$text, collapse = ". ")`)

#Biden analysis
biden_all_tweet <- read_csv("data/biden_after.csv")

#get only the tweets after the election
biden_after_analysis <- biden_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date >= "2020-11-03",
        display_text_width > 100)

#get the tweets before the election
biden_before_analysis <- biden_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date < "2020-11-03",
        display_text_width > 100)

#add variable before or after
biden_after_analysis <-biden_after_analysis %>% 
  mutate(when = "after")

biden_before_analysis <-biden_before_analysis %>% 
  mutate(when = "before")

biden_after_analysis <- biden_after_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(biden_after_analysis$text, collapse = '. ')) 

biden_after_analysis <- rename(biden_after_analysis, text = `paste(biden_after_analysis$text, collapse = ". ")`)
  
biden_before_analysis <- biden_before_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(biden_before_analysis$text, collapse = '. ')) 

biden_before_analysis <- rename(biden_before_analysis, text = `paste(biden_before_analysis$text, collapse = ". ")`)

#Kamala Harris analysis
kamala_all_tweet <- read_csv("data/kamala_after.csv")

#get only the tweets after the election
kamala_after_analysis <- kamala_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date >= "2020-11-03",
        display_text_width > 100)

#get the tweets before the election
kamala_before_analysis <- kamala_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date < "2020-11-03",
        display_text_width > 100)

#add variable before or after
kamala_after_analysis <-kamala_after_analysis %>% 
  mutate(when = "after")

kamala_before_analysis <-kamala_before_analysis %>% 
  mutate(when = "before")

kamala_after_analysis <- kamala_after_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(kamala_after_analysis$text, collapse = '. ')) 

kamala_after_analysis <- rename(kamala_after_analysis, text = `paste(kamala_after_analysis$text, collapse = ". ")`)
  
kamala_before_analysis <- kamala_before_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(kamala_before_analysis$text, collapse = '. ')) 

kamala_before_analysis <- rename(kamala_before_analysis, text = `paste(kamala_before_analysis$text, collapse = ". ")`)

#Obama analysis
obama_all_tweet <- read_csv("data/obama_after.csv")

#get only the tweets after the election
obama_after_analysis <- obama_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date >= "2020-11-03",
        display_text_width > 100)

#get the tweets before the election
obama_before_analysis <- obama_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date < "2020-11-03",
        display_text_width > 100)

#add variable before or after
obama_after_analysis <-obama_after_analysis %>% 
  mutate(when = "after")

obama_before_analysis <-obama_before_analysis %>% 
  mutate(when = "before")

obama_after_analysis <- obama_after_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(obama_after_analysis$text, collapse = '. ')) 

obama_after_analysis <- rename(obama_after_analysis, text = `paste(obama_after_analysis$text, collapse = ". ")`)
  
obama_before_analysis <- obama_before_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(obama_before_analysis$text, collapse = '. ')) 

obama_before_analysis <- rename(obama_before_analysis, text = `paste(obama_before_analysis$text, collapse = ". ")`)

#AOC analysis
aoc_all_tweet <- read_csv("data/aoc_after.csv")

#get only the tweets after the election
aoc_after_analysis <- aoc_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date >= "2020-11-03",
        display_text_width > 100)

#get the tweets before the election
aoc_before_analysis <- aoc_all_tweet %>% 
  mutate(date = date(created_at)) %>% 
  filter(date < "2020-11-03",
        display_text_width > 100)

#add variable before or after
aoc_after_analysis <-aoc_after_analysis %>% 
  mutate(when = "after")

aoc_before_analysis <-aoc_before_analysis %>% 
  mutate(when = "before")

aoc_after_analysis <- aoc_after_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(aoc_after_analysis$text, collapse = '. ')) 

aoc_after_analysis <- rename(aoc_after_analysis, text = `paste(aoc_after_analysis$text, collapse = ". ")`)
  
aoc_before_analysis <- aoc_before_analysis %>% 
  select(text,screen_name, when) %>% 
  group_by(screen_name, when) %>% 
  summarise(paste(aoc_before_analysis$text, collapse = '. ')) 

aoc_before_analysis <- rename(aoc_before_analysis, text = `paste(aoc_before_analysis$text, collapse = ". ")`)

corpus <- rbind(Trump_after_analysis, Trump_before_analysis, vp_after_analysis, vp_before_analysis,
                biden_after_analysis, biden_before_analysis, kamala_after_analysis, kamala_before_analysis,
                obama_after_analysis, obama_before_analysis, aoc_after_analysis, aoc_before_analysis)
corpus <- corpus %>% mutate(Document=paste(when,"_",screen_name,sep="")) 

#creation of the corpus
corpus = data.frame(lapply(corpus, as.character), stringsAsFactors=FALSE)

write.csv(corpus, "data\\corpus.csv", row.names=FALSE)

```

#### Cleaning of the corpus

```{r}
corpus <- read_csv("data/corpus.csv")
corpus <- corpus %>% mutate(Document=paste(when,"_",screen_name,sep="")) 

#set the text to lowercase
corpus$text <- tolower(corpus$text)

#remove mentions, urls, emojis, numbers, punctuations, etc.
corpus$text <- gsub("@\\w+", "", corpus$text)
corpus$text <- gsub("\\d+\\w*\\d*", "", corpus$text)
corpus$text <- gsub("#\\w+", "", corpus$text)
corpus$text <- gsub("[^\x01-\x7F]", "", corpus$text)
corpus$text <- gsub("[[:punct:]]", " ", corpus$text)

corpus_final = data.frame(lapply(corpus, as.character), stringsAsFactors=FALSE)

write.csv(corpus_final, "data\\corpus_final_before_after.csv", row.names=FALSE)
```

### Global corpus (all tweets)

```{r}
#Trump analysis
TRUMP_all_tweet <- read_csv("data/trump_after.csv")

TRUMP_all_tweet <- TRUMP_all_tweet %>% 
  select(text,screen_name) %>% 
  group_by(screen_name) %>% 
  summarise(paste(TRUMP_all_tweet$text, collapse = ' ')) 

TRUMP_all_tweet <- rename(TRUMP_all_tweet, text = `paste(TRUMP_all_tweet$text, collapse = " ")`)

#Vice president analysis
vp_all_tweet <- read_csv("data/vp_after.csv")

vp_all_tweet <- vp_all_tweet %>% 
  select(text,screen_name) %>% 
  group_by(screen_name) %>% 
  summarise(paste(vp_all_tweet$text, collapse = ' ')) 

vp_all_tweet <- rename(vp_all_tweet, text = `paste(vp_all_tweet$text, collapse = " ")`)

#Biden analysis
biden_all_tweet <- read_csv("data/biden_after.csv")

biden_all_tweet <- biden_all_tweet %>% 
  select(text,screen_name) %>% 
  group_by(screen_name) %>% 
  summarise(paste(biden_all_tweet$text, collapse = ' ')) 

biden_all_tweet <- rename(biden_all_tweet, text = `paste(biden_all_tweet$text, collapse = " ")`)
  
#Kamala Harris analysis
kamala_all_tweet <- read_csv("data/kamala_after.csv")

kamala_all_tweet <- kamala_all_tweet %>% 
  select(text,screen_name) %>% 
  group_by(screen_name) %>% 
  summarise(paste(kamala_all_tweet$text, collapse = ' ')) 

kamala_all_tweet <- rename(kamala_all_tweet, text = `paste(kamala_all_tweet$text, collapse = " ")`)

#Obama analysis
obama_all_tweet <- read_csv("data/obama_after.csv")

obama_all_tweet <- obama_all_tweet %>% 
  select(text,screen_name) %>% 
  group_by(screen_name) %>% 
  summarise(paste(obama_all_tweet$text, collapse = ' ')) 

obama_all_tweet <- rename(obama_all_tweet, text = `paste(obama_all_tweet$text, collapse = " ")`)

#AOC analysis
aoc_all_tweet <- read_csv("data/aoc_after.csv")

aoc_all_tweet <- aoc_all_tweet %>% 
  select(text,screen_name) %>% 
  group_by(screen_name) %>% 
  summarise(paste(aoc_all_tweet$text, collapse = ' ')) 

aoc_all_tweet <- rename(aoc_all_tweet, text = `paste(aoc_all_tweet$text, collapse = " ")`)

corpus_all <- rbind(TRUMP_all_tweet, vp_all_tweet, biden_all_tweet, kamala_all_tweet, obama_all_tweet, aoc_all_tweet)

#creation of the global corpus
corpus_all = data.frame(lapply(corpus_all, as.character), stringsAsFactors=FALSE)

write.csv(corpus_all, "data\\corpus_final.csv", row.names=FALSE)

```

#### Cleaning the corpus

```{r}
corpus_final <- read_csv("data/corpus_final.csv")

#set the text to lowercase
corpus_final$text <- tolower(corpus_final$text)

#remove mentions, urls, emojis, numbers, punctuations, etc.
corpus_final$text <- gsub("@\\w+", "", corpus_final$text)
corpus_final$text <- gsub("\\d+\\w*\\d*", "", corpus_final$text)
corpus_final$text <- gsub("#\\w+", "", corpus_final$text)
corpus_final$text <- gsub("[^\x01-\x7F]", "", corpus_final$text)
corpus_final$text <- gsub("[[:punct:]]", " ", corpus_final$text)

corpus_final = data.frame(lapply(corpus_final, as.character), stringsAsFactors=FALSE)

write.csv(corpus_final, "data\\corpus_final.csv", row.names=FALSE)
```

# Sentiment analysis

```{r message=FALSE, warning=FALSE}
corpus_final_before_after <- read_csv("data/corpus_final_before_after.csv")
corpus_final <- read_csv("data/corpus_final.csv")

tweet.tb <- as_tibble(data.frame(corpus_final))
tweet.tok <- unnest_tokens(tweet.tb, output="word", input="text", to_lower=TRUE, strip_punct=TRUE, 
                           strip_numeric=TRUE)

tweet.sent <- tweet.tok %>%
  inner_join(get_sentiments("nrc"))

tweet.sent %>% group_by(screen_name, sentiment) %>% summarize(n = n()) %>% 
  ggplot(aes(x = sentiment, y = n, fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ screen_name) + coord_flip()

tweet.sent %>% group_by(screen_name, sentiment) %>%  summarize(n = n()) %>% 
  mutate(freq = n / sum(n)) %>% ggplot(aes(x = sentiment, y = freq, fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ screen_name) + coord_flip()

#scaling with the length
tweet.tb <- as_tibble(data.frame(corpus_final_before_after))
tweet.tok <- unnest_tokens(tweet.tb, output="word", input="text", to_lower=TRUE, strip_punct=TRUE, 
                           strip_numeric=TRUE)

tweet.sent <- tweet.tok %>%
  inner_join(get_sentiments("nrc"))

tweet.sent %>% group_by(Document, sentiment) %>%  summarize(n = n()) %>% 
  mutate(freq = n / sum(n)) %>% ggplot(aes(x = sentiment, y = freq, fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ Document) + coord_flip()

```

## Value based (afinn)
```{r message=FALSE, warning=FALSE}
tweet.sent <- tweet.tok %>%
  inner_join(get_sentiments("afinn"))

#summarize per document (value average) + barplot
aggregate(value~screen_name, data=tweet.sent, FUN=mean)

aggregate(value~screen_name, data=tweet.sent, FUN=mean) %>% 
  ggplot(aes(x = screen_name, y = value)) + 
  geom_bar(stat = "identity") + coord_flip()

#before and after the election day
tweet.tb <- as_tibble(data.frame(corpus_final_before_after))
tweet.tok <- unnest_tokens(tweet.tb, output="word", input="text", to_lower=TRUE, strip_punct=TRUE, 
                           strip_numeric=TRUE)

tweet.sent <- tweet.tok %>%
  inner_join(get_sentiments("afinn"))

#summarize per document (value average) + barplot
aggregate(value~Document, data=tweet.sent, FUN=mean)

aggregate(value~Document, data=tweet.sent, FUN=mean) %>% 
  ggplot(aes(x = Document, y = value)) + 
  geom_bar(stat = "identity") + coord_flip()


```

## Valence shifter

It is not working with our dataset.

```{r}
library(sentimentr)
library(lexicon)

corpus_final <- read_csv("data/corpus_final.csv")

corpus_valence <- corpus_final %>% head(2)

tweet.text <- get_sentences(corpus_valence$text)
(tweet.senti <- sentiment(tweet.text))

tweet.senti <- as_tibble(tweet.senti)

tweet.senti %>% group_by(element_id) %>%
    ggplot(aes(x = sentence_id, y = sentiment)) +
    geom_line() +
    facet_wrap( ~ element_id) 
```

# Similarities

## Jaccard index

```{r message=FALSE, warning=FALSE}
corpus_final <- read_csv("data/corpus_final_before_after.csv")

#create the TI-IDF matrix and remove some words 
tweet.cp <- corpus(corpus_final$text, corpus_final$Document)
tweet.tfidf <- dfm_tfidf(dfm(tweet.cp, remove_punct = TRUE, remove_numbers=TRUE, remove = c("u","polowczyk", "xdzz","dylan", "amp", "tune", "garland", "t","co", "https","gsfsgh","et", "http", "gsfsghkmdm", "eoxt", stopwords("english"))))

#jaccard index
TW_jacc <- textstat_simil(tweet.tfidf, method = "jaccard", margin = "documents")

#heatmap
library(reshape)
TW.jac.mat <- melt(as.matrix(TW_jacc)) # Convert the object to matrix then to data frame 
ggplot(data = TW.jac.mat, aes(x=X1, y=X2, fill=value))+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0.5, limit = c(0,1), name="Jaccard")+
  geom_tile()

#clustering
TW_hc <- hclust(dist(1 - TW_jacc))

#plot
plot(TW_hc)

#explain the cluster
tweet.clust <- cutree(TW_hc, k=3)
tweet.clust <- as.data.frame(tweet.clust) %>% arrange(desc(tweet.clust))
tweet.clust%>% 
  kable(caption = "General statistics", col.names = c()) %>%
  kable_styling(
    bootstrap_options = "striped",
    full_width = F,
    position = "left"
  )

#extract the most used words
tweet.clust <- cutree(TW_hc, k=3)
wordsclust <- data.frame(
  Clust.1 = names(sort(apply(tweet.tfidf[tweet.clust==1,],2,sum), decreasing = TRUE)[1:8]),
  Clust.2 = names(sort(apply(tweet.tfidf[tweet.clust==2,],2,sum), decreasing = TRUE)[1:8]),
  Clust.3 = names(sort(apply(tweet.tfidf[tweet.clust==3,],2,sum), decreasing = TRUE)[1:8])) 

wordsclust%>% 
  kable(caption = "clustering", col.names = c("cluster 1", "cluster 2", "cluster 3")) %>%
  kable_styling(
    bootstrap_options = "striped",
    full_width = F,
    position = "left"
  )
```

# Topic modeling

In this part, we use the database containing tweets from before and after the election day. When creating the corpus, we regroup all tweets from a politican in a same document. As we have six politicians, we have six documents in our corpus. 

We proceed to the cleaning of the corpus. 

```{r message=FALSE, warning=FALSE}
corpus_final <- read_csv("data/corpus_final.csv")
corpus_final <- corpus(corpus_final$text, corpus_final$screen_name)
corpus_final <- gsub('\\b\\w{1,2}\\b','', corpus_final) #remove words whose length is smaller than 3

library(quanteda)
library(quanteda.textmodels)
tweet.dfm <- dfm(corpus_final,
                 remove_punct = TRUE, 
                 remove_symbols = TRUE, 
                 remove_url = TRUE,
                 remove = stopwords("english"),
                 remove_numbers=TRUE)
```

## LSA

```{r message=FALSE, warning=FALSE}
dimsetup <- textmodel_lsa(tweet.dfm, nd=4)

#analyze the correlation
ns <- apply(tweet.dfm, 1, sum) # row-sum of the DTM
plot(ns~dimsetup$docs[,1], xlab = "Dimension 1", ylab = "Document length", main = "Dimension 1 versus document lenght")
```

### LSA with TF 

```{r message=FALSE, warning=FALSE}
#plot dimensions 2 and 3 
x <- dimsetup$features[,2:3] %>% as.data.frame()
row.names.remove <- c("u","t","co ","https","http","s","amp","re","m","ll", "co", "we", "obama1")
x <- x[!(row.names(x) %in% row.names.remove), ]

x1 <- x %>%
          filter(V1>0.06 | V2>0.06)

x2 <- x %>%
          filter(V1< -0.06 | V2< -0.06) 

x <- rbind(x1, x2, by=0, all=TRUE)
x <- x[!(row.names(x) %in% row.names.remove), ]

biplot(y= dimsetup$docs[,2:3],x= x, col=c("grey","red"),
       xlab = "Dim 2", ylab="Dim 3",expand=2, xlim=c(-0.7,0.9), ylim=c(-0.6,0.99))

#plot dimensions 3 and 4
x <- dimsetup$features[,3:4] %>% as.data.frame()

x1 <- x %>%
          filter(V1>0.06 | V2>0.06)

x2 <- x %>%
          filter(V1< -0.06 | V2< -0.06) 

x <- rbind(x1, x2, by=0, all=TRUE)
x <- x[!(row.names(x) %in% row.names.remove), ]

biplot(y= dimsetup$docs[,3:4],x= x, col=c("grey","red"),
       xlab = "Dim 3", ylab="Dim 4",expand=1, xlim=c(-0.4,0.9), ylim=c(-0.4, 0.8))

#extract
row.names.remove <- c("u","t","co ","https","http","s","amp","re","m","ll", "co", "we", "obama1")
dimsetup$features <- dimsetup$features[!(row.names(dimsetup$features) %in% row.names.remove), ]

n.terms <- 10

#extract words in each topic
#topic 1
sort(abs(dimsetup$features[,1]), decreasing = TRUE)[1:n.terms] %>% kable(col.names = NULL, caption = "Main words of topic 1") %>% kable_styling(latex_options = "striped",full_width = FALSE)

#topic 2
t(t(sort(dimsetup$features[, 2], decreasing = TRUE)[c(1:n.terms, nrow(dimsetup$features) -
                                                        c(n.terms:1) + 1)])) %>% kable(col.names = NULL, caption = "Main words of topic 2") %>% kable_styling(latex_options = "striped",full_width = FALSE)
#topic 3
t(t(sort(dimsetup$features[, 3], decreasing = TRUE)[c(1:n.terms, nrow(dimsetup$features) -
                                                        c(n.terms:1) + 1)])) %>% kable(col.names = NULL, caption = "Main words of topic 3") %>% kable_styling(latex_options = "striped",full_width = FALSE)
#topic 4
t(t(sort(dimsetup$features[, 4], decreasing = TRUE)[c(1:n.terms, nrow(dimsetup$features) -
                                                        c(n.terms:1) + 1)])) %>% kable(col.names = NULL, caption = "Main words of topic 4") %>% kable_styling(latex_options = "striped",full_width = FALSE)
```

### LSA with TF-IDF

We repeat the same LSA process but with TF-IDF.

```{r}
tweets.tfidf <- dfm_tfidf(tweet.dfm)
dimsetup <- textmodel_lsa(tweets.tfidf, nd=4)

#analyze the correlation
ns <- apply(tweets.tfidf, 1, sum) # row-sum of the DTM
plot(ns~dimsetup$docs[,1])

#plot dimensions 2 and 3 
x <- dimsetup$features[,2:3] %>% as.data.frame()
row.names.remove <- c("u","t","co ","https","http","s","amp","re","m","ll", "co", "we", "obama1", "all")
x <- x[!(row.names(x) %in% row.names.remove), ]

x1 <- x %>%
          filter(V1>0.06 | V2>0.06)

x2 <- x %>%
          filter(V1< -0.06 | V2< -0.06) 

x <- rbind(x1, x2, by=0, all=TRUE)
x <- x[!(row.names(x) %in% row.names.remove), ]

biplot(y= dimsetup$docs[,2:3],x= x, col=c("grey","red"),
       xlab = "Dim 2", ylab="Dim 3", expand = 1)

#plot dimensions 3 and 4
x <- dimsetup$features[,3:4] %>% as.data.frame()

x1 <- x %>%
          filter(V1>0.06 | V2>0.06)

x2 <- x %>%
          filter(V1< -0.06 | V2< -0.06) 

x <- rbind(x1, x2, by=0, all=TRUE) 
x <- x[!(row.names(x) %in% row.names.remove), ]

biplot(y= dimsetup$docs[,3:4],x= x, col=c("grey","red"),
       xlab = "Dim 3", ylab="Dim 4",expand=1)

#extract
row.names.remove <- c("u","t","co ","https","http","s","amp","re","m","ll", "co", "we", "obama1", "xdzz", "jill", "eoxt", "usmca", "ofa", "uoivh")
dimsetup$features <- dimsetup$features[!(row.names(dimsetup$features) %in% row.names.remove), ]

n.terms <- 10

#extract words in each topic
#topic 1
sort(abs(dimsetup$features[,1]), decreasing = TRUE)[1:n.terms] %>% kable(col.names = NULL, caption = "Main words of topic 1") %>% kable_styling(latex_options = "striped",full_width = FALSE)

#topic 2
t(t(sort(dimsetup$features[, 2], decreasing = TRUE)[c(1:n.terms, nrow(dimsetup$features) -
                                                        c(n.terms:1) + 1)])) %>% kable(col.names = NULL, caption = "Main words of topic 2") %>% kable_styling(latex_options = "striped",full_width = FALSE)
#topic 3
t(t(sort(dimsetup$features[, 3], decreasing = TRUE)[c(1:n.terms, nrow(dimsetup$features) -
                                                        c(n.terms:1) + 1)])) %>% kable(col.names = NULL, caption = "Main words of topic 3") %>% kable_styling(latex_options = "striped",full_width = FALSE)
#topic 4
t(t(sort(dimsetup$features[, 4], decreasing = TRUE)[c(1:n.terms, nrow(dimsetup$features) -
                                                        c(n.terms:1) + 1)])) %>% kable(col.names = NULL, caption = "Main words of topic 4") %>% kable_styling(latex_options = "striped",full_width = FALSE)
```

## LDA

```{r message=FALSE, warning=FALSE}
library(topicmodels)
K <- 8
tweet.dtm <- convert(tweet.dfm, to = "topicmodels")

lda <- LDA(tweet.dtm, k = K) 

terms <- terms(lda, 10)[-1,] #remove the first row composed of https word
terms %>% kable(caption = "The words most associated with the topics") %>% kable_styling(latex_options = "striped",full_width = FALSE)

topics(lda, 1) %>% kable(col.names = NULL, caption = "Topics most associated with each document") %>% kable_styling(latex_options = "striped", full_width = FALSE)

#show the betas of each document
beta.td <- tidy(lda, matrix = "beta") # beta's are turned to proba scales
beta.td 

badwords <- c("u","t","co ","https","http","s","amp","re","m","ll", "co", "we", "obama1")
beta.td <- beta.td[ !grepl(paste(badwords, collapse="|"), beta.td$term),]

#describe the topics with their most associated terms
beta.top.terms <- beta.td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

beta.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Topics with their most associated terms") + ylab("Beta") +
  xlab("Terms") 

#describe the topics in each document
gamma.td <- tidy(lda, matrix = "gamma")
gamma.td %>%
  ggplot(aes(document, gamma, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() + labs(title = "Topics with the most associated documents") + ylab("Gamma") +
  xlab("Documents") 
```


# Supervised learning

In this part, we will use the whole set of data which includes the tweets before and after the election. We would like to predict the person who wrote those tweets and also his political party. 

At the beginning, we add the variable corresponding to the political party of the tweet's writer. We also create a new variable with the abbreviation of the name of the tweet's writers. Then, we create a corpus and clean it. We use the lemmatization method in the tokenisation process. 

```{r include=FALSE}
f <- file.path(here("data/"), c("biden_after.csv","kamala_after.csv", "trump_after.csv", "vp_after.csv", "obama_after.csv", "aoc_after.csv"))
d <- lapply(f, read.csv)

tweet_all <- rbind(d[[1]], d[[2]], d[[3]], d[[4]],d[[5]],d[[6]])

tweet_all <- tweet_all %>% select(screen_name, created_at, text, source, display_text_width, favorite_count, retweet_count)

#add the political party of the author of a tweet to the dataset
tweet_all <- tweet_all %>% mutate(party = case_when(grepl(c("realDonaldTrump|VP"), screen_name) ~ "republican", 
                                    grepl(c("BarackObama|JoeBiden|KamalaHarris|AOC"), screen_name) ~ "democrate"))

tweet_all$text <- as.character(tweet_all$text) #transform text column into character

tweets.corpus <- corpus(tweet_all)

tweets.corpus <- gsub('\\b\\w{1,2}\\b','', tweets.corpus) #remove tokens whose length is smaller than 3

#tokenisation
tweets.tokens <- tokens(
  tweets.corpus,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  what = "word1")

 tweets.tokens <- tweets.tokens %>% tokens_replace(pattern = hash_lemmas$token, replacement =
                                           hash_lemmas$lemma)  #use the lemmatization 

tweets.tokens <- tweets.tokens %>% tokens_tolower() %>% tokens_remove(stopwords("english")) %>% tokens_remove(c("https", "t.co", "http", "amp"))
 
tweets.tokens <- tweets.tokens %>%  tokens_remove(pattern = "(?<=\\d{1,9})\\w+", valuetype = "regex" ) #remove tokens containing numbers

tweets.tokens <- tweets.tokens %>% tokens_subset(ntoken(tweets.tokens) > 2) #remove sentences with less than 3 tokens
```

### Word embedding

We create a words vector representation using the GloVe method. To do it, we first need to compute the co-occurence matrix. 

```{r}
tweets_coo <- fcm(tweets.tokens, context = "window", window = 5, tri = FALSE)

library(text2vec)
p <- 2 #word embedding dimension
tweets.glove <- GlobalVectors$new(rank = p, x_max = 10) #GloVe method
tweets.we <- tweets.glove$fit_transform(tweets_coo) #central vectors
word_vectors_context <- tweets.glove$components #context vectors
twitter.glove <- tweets.we + t(word_vectors_context) #match central vectors and context vectors

nwords <- apply(dfm(tweets.tokens), 2, sum) #number of times each term is used (frequency)
index <- order(nwords, decreasing = TRUE)[1:100] #100 largest number of words
plot(tweets.we[index,], type ="n", xlab="Dim1", ylab="Dim2", main = "The 100 most frequent words") #plot the 100 most frequent words in the corpus

text(x=tweets.we[index,], labels = rownames(tweets.we[index,]))
```

Because the cluster is completely illegible, we display the 10 most frequent words in 5 clusters. Clusters are computed using the RelaxedWordMoversDistance (RWMD)

```{r}
tweets.dtm <- dfm(tweets.tokens)
tweets.rwmd <- RelaxedWordMoversDistance$new(tweets.dtm, tweets.we)

tweets.rwmd.dist <- tweets.rwmd$dist2(tweets.dtm)
tweets.hc <- hclust(as.dist(tweets.rwmd.dist))   

memb <- cutree(tweets.hc, k = 5) #to have a significant clustering, we reduce their numbers to 5 

tweets.dtm[memb == 1,]
data.frame(Clust.1 = names(sort(apply(tweets.dtm[memb == 1,], 2, sum), decreasing = TRUE)[1:10]),
           Clust.2 = names(sort(apply(tweets.dtm[memb == 2,], 2, sum), decreasing = TRUE)[1:10]),
           Clust.3 = names(sort(apply(tweets.dtm[memb == 3,], 2, sum), decreasing = TRUE)[1:10]),
           Clust.4 = names(sort(apply(tweets.dtm[memb == 4,], 2, sum), decreasing = TRUE)[1:10]),
           Clust.5 = names(sort(apply(tweets.dtm[memb == 5,], 2, sum), decreasing = TRUE)[1:10])) 
```

Below, we repeat the same process but with 25 word embedding dimensions. To illustrate it, we display the 10 most frequent words in 5 clusters. 

```{r}
p <- 25 #word embedding dimension
tweets.glove <- GlobalVectors$new(rank = p, x_max = 10) #GloVe method
tweets.we <- tweets.glove$fit_transform(tweets_coo) #central vectors
word_vectors_context <- tweets.glove$components #context vectors
twitter.glove <- tweets.we + t(word_vectors_context) #match central vectors and context vectors

nwords <- apply(dfm(tweets.tokens), 2, sum) #number of times each term is used (frequency)
index <- order(nwords, decreasing = TRUE)[1:100] #100 largest number of words

ndoc <- length(tweets.tokens) #number of documents in the corpus 
centers <- matrix(nrow = ndoc, ncol = p) #create a matrix for the centroids
for(i in 1:ndoc){ #compute the centroids 
  words_in_i <- twitter.glove[tweets.tokens[[i]],, drop = FALSE]
  centers[i,] <- apply(words_in_i, 2, mean)
}

row.names(centers) <- names(tweets.tokens) #name the columns of the centroid matrix

tweets.dtm <- dfm(tweets.tokens) #TF
tweets.rwmd <- RelaxedWordMoversDistance$new(tweets.dtm, tweets.we)

tweets.rwmd.dist <- tweets.rwmd$dist2(tweets.dtm)
tweets.hc <- hclust(as.dist(tweets.rwmd.dist))

memb <- cutree(tweets.hc, k = 5) #to have a significant clustering, we reduce their numbers to 5 

tweets.dtm[memb == 1,]
data.frame(Clust.1 = names(sort(apply(tweets.dtm[memb == 1,], 2, sum), decreasing = TRUE)[1:10]),
           Clust.2 = names(sort(apply(tweets.dtm[memb == 2,], 2, sum), decreasing = TRUE)[1:10]),
           Clust.3 = names(sort(apply(tweets.dtm[memb == 3,], 2, sum), decreasing = TRUE)[1:10]),
           Clust.4 = names(sort(apply(tweets.dtm[memb == 4,], 2, sum), decreasing = TRUE)[1:10]),
           Clust.5 = names(sort(apply(tweets.dtm[memb == 5,], 2, sum), decreasing = TRUE)[1:10]))
```

As we can see, the clustering seems more significant. For example, cluster 5 links gathers words that seem to be of Spanish origin, cluster 3 links words related to joy (birthday, happy).

### Author of the tweet prediction

We will build different models to predict the author of a tweet (y = screen_name) and compare them. 

#### TF + LSA 

```{r include=FALSE}
y <- factor(docvars(tweets.tokens, "screen_name")) #who wrote the tweet is the variable to predict

tweets.dfm <- dfm(tweets.tokens)

set.seed(2020)
index.tr <- sample(size=round(0.8*length(y)), x=c(1:length(y)), replace=FALSE) #we split the data into a training set and a test set

nd.vec <- c(2,5,25,50,100, 500, 1000) #because our matrix is well too big, we apply a reduction dimension technique such as the LSA one

acc.vec <- numeric(length(nd.vec))
for (j in 1:length(nd.vec)){
  tweets.lsa <- textmodel_lsa(tweets.dfm, nd=nd.vec[j])
  df <- data.frame(Class=y, X=tweets.lsa$docs)
  df <- cbind(df, 
            logretweet=log10(docvars(tweets.tokens, c("retweet_count"))),
            length = log(sapply(tweets.tokens, length)))
  df.tr <- df[index.tr,]
  df.te <- df[-index.tr,]
  set.seed(2020)
  tweets.fit <- ranger(Class ~ ., #use a random forest
                       data = df.tr)
  pred.te <- predict(tweets.fit, df.te)
  acc.vec[j] <- confusionMatrix(data=pred.te$predictions, reference = df.te$Class)$overall[1]
}

acc.vec

plot(acc.vec ~ nd.vec, type='b', main = "LSA on a DTM matrix composed by the word frequency", ylab = "Accuracy", xlab = "Number of LSA dimensions") #500 dimensions has the best accuracy
```

We found an accuracy of 0.6861423. This score is related to a LSA of 500 dimensions. 

#### TF-IDF + LSA

```{r}
tweets.tfidf <- dfm_tfidf(tweets.dfm)

nd.vec <- c(2,5,25,50,100, 500, 1000) #because our matrix is well too big, we apply a reduction dimension technique such as the LSA one

acc.vec <- numeric(length(nd.vec))
for (j in 1:length(nd.vec)){
  tweets.lsa <- textmodel_lsa(tweets.tfidf, nd=nd.vec[j])
  df <- data.frame(Class=y, X=tweets.lsa$docs)
   df <- cbind(df, 
            logretweet=log10(docvars(tweets.tokens, c("retweet_count"))),
            length = log(sapply(tweets.tokens, length)))
  df.tr <- df[index.tr,]
  df.te <- df[-index.tr,]
  set.seed(2020)
  tweets.fit <- ranger(Class ~ ., #use a random forest
                       data = df.tr)
  pred.te <- predict(tweets.fit, df.te)
  acc.vec[j] <- confusionMatrix(data=pred.te$predictions, reference = df.te$Class)$overall[1]
}

acc.vec
plot(acc.vec ~ nd.vec, type='b', main = "LSA with TF-IDF", ylab = "Accuracy", xlab = "Number of LSA dimensions") #50 dimensions has the best accuracy
```

As we can see, accuracy is better when using TF-IDF with LSA of 50 dimensions (0.7382022). 

#### Word embedding + centroids

We compute the centroids in order to transform the word embedding into a document embedding. 

We start with the two dimensions word embedding from the previous part. 

```{r}
p <- 2 #word embedding dimension
tweets.glove <- GlobalVectors$new(rank = p, x_max = 10) #GloVe method
tweets.we <- tweets.glove$fit_transform(tweets_coo) #central vectors
word_vectors_context <- tweets.glove$components #context vectors
twitter.glove <- tweets.we + t(word_vectors_context) #match central vectors and context vectors

ndoc <- length(tweets.tokens) #number of documents in the corpus 
centers <- matrix(nrow = ndoc, ncol = p) #create a matrix for the centroids
for(i in 1:ndoc){ #compute the centroids 
  words_in_i <- twitter.glove[tweets.tokens[[i]],, drop = FALSE]
  centers[i,] <- apply(words_in_i, 2, mean)
}

row.names(centers) <- names(tweets.tokens) #name the columns of the centroid matrix

plot(x = centers, type ="n", xlab="Dim1", ylab="Dim2", main = "Centroids") #plot the centroids
text(centers, labels = rownames(centers))
```

```{r}
df <- data.frame(Class=y, X=centers)
df.tr <- df[index.tr,]
df.te <- df[-index.tr,]
set.seed(2020)

tweets.fit <- ranger(Class ~ .,
                     data = df.tr)

pred.te <- predict(tweets.fit, df.te)
confusionMatrix(data=pred.te$predictions, reference = df.te$Class)
```

With an accuracy of 0.2764, the cendroids coming from the word embedding part (two dimensions word embedding) are very bad for the prediction model. We could try to use more word embedding dimensions in order to see if we get better results. We set the dimensions of word embedding to 25. 

```{r}
tweets_coo <- fcm(tweets.tokens, context = "window", window = 5, tri = FALSE)
p <- 25 #word embedding dimension
tweets.glove <- GlobalVectors$new(rank = p, x_max = 10) #GloVe method
tweets.we <- tweets.glove$fit_transform(tweets_coo) #central vectors
word_vectors_context <- tweets.glove$components #context vectors
twitter.glove <- tweets.we + t(word_vectors_context) #match central vectors and context vectors

ndoc <- length(tweets.tokens) #number of documents in the corpus 
centers <- matrix(nrow = ndoc, ncol = p) #create a matrix for the centroids
for(i in 1:ndoc){ #compute the centroids 
  words_in_i <- twitter.glove[tweets.tokens[[i]],, drop = FALSE]
  centers[i,] <- apply(words_in_i, 2, mean)
}

row.names(centers) <- names(tweets.tokens) #name the columns of the centroid matrix

df <- data.frame(Class=y, X=centers)
df.tr <- df[index.tr,]
df.te <- df[-index.tr,]

set.seed(2020)
tweets.fit <- ranger(Class ~ ., 
                     data = df.tr)

pred.te <- predict(tweets.fit, df.te)
cm1 <- confusionMatrix(data=pred.te$predictions, reference = df.te$Class)

draw_confusion_matrix <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Class1', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Class2', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Class1', cex=1.2, srt=90)
  text(140, 335, 'Class2', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  # text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  # text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  # text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  # text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  # text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  # text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  # text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  # text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  # text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  # text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 60, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 35, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 60, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 35, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  

draw_confusion_matrix(cm1)
```

With 25 dimensions, we get an accuracy of 0.621 which is better than when using only two dimensions in word embedding. We will keep using 25 dimensions in order to limit the computational time for the following.

TF-IDF is doing better. Now, we will try to combine the centroids with TF-IDF in order to see if we can improve accuracy. 

#### Combining centroids with TF-IDF + LSA 


```{r}
tweets.tfidf <- dfm_tfidf(tweets.dfm) #TF-IDF
tweets.lsa <- textmodel_lsa(tweets.tfidf, nd=50) 
df <- data.frame(Class=y, X=tweets.lsa$docs)
df <- cbind(df, 
            logretweet=log10(docvars(tweets.tokens, c("retweet_count"))),
            length = log(sapply(tweets.tokens, length)))
df <- cbind(df, Cent=centers) #add the centroid from word embedding part
df.tr <- df[index.tr,]
df.te <- df[-index.tr,]
set.seed(2020)
tweets.fit <- ranger(Class ~ ., 
                     data = df.tr, 
                     importance = "impurity")
pred.te <- predict(tweets.fit, df.te)
cm2 <- confusionMatrix(data=pred.te$predictions, reference = df.te$Class)
draw_confusion_matrix(cm2)
```

The combination of features (TF-IDF + LSA + Centroids) (0.7367) does not surpass the predictions made with the TF-IDF and LSA (0.7382022). 

For the following, we will keep this last combination of features (TF-IDF + LSA + Centroids) to compare the results with different models.

#### Support vector machine (SVM) with centroids + TF-IDF + LSA 

```{r}
is.na(df) <- sapply(df, is.infinite) #transform infinite values into NA 
df<- drop_na(df) #remove na values
df.tr <- df[index.tr,]
df.te <- df[-index.tr,]

train_control <- trainControl(method = "cv", number = 5)
metric <- "Accuracy"

set.seed(2020)
fit_svm <- train(
  form = Class ~ .,
  data = df.tr,
  trControl = train_control,
  tuneLength = 5,
  method = "svmRadial",
  preProcess = c("center","scale"),
  metric = metric,
  na.action=na.exclude
)

pred.te <- predict(tweets.fit, df.te)
cm3 <-  confusionMatrix(data=pred.te$predictions, reference = df.te$Class)
draw_confusion_matrix(cm3)
```

The SVM model, with an accuracy of 0.8472, is doing a great job in comparison to the random forest model.

#### Neural network (NN) with centroids + TF-IDF + LSA 

In order to avoid overfitting, we will use a weight decay to penalise the largest weights. We will also tune the size of the neural networks to find the opimal one.

We will use a five cross-validation to subset the training set (into a validation set) in order to asses how well the model will generalize to the test set.

```{r}
hp_nn <- expand.grid(size = 2:10,
                     decay = seq(0, 0.5, 0.05))

train_control <- trainControl(method = "cv", number = 5)
metric <- "Accuracy"

set.seed(2020)
tweets.fit <- train(
  form = Class ~ .,
  data = df.tr,
  tuneGrid = hp_nn,
  method = "nnet",
  trControl = train_control,
  metric = metric,
  na.action=na.exclude)

pred.te <- predict(tweets.fit, newdata = df.te)
cm4 <- confusionMatrix(data=pred.te, reference = df.te$Class)
draw_confusion_matrix(cm4)
```

The accuracy of 0.671 is really disappointing. The neural network model should not be used to predict the author of a tweet. 

#### Linear discriminant analysis (LDA) with centroids + TF-IDF + LSA 

```{r}
train_control <- trainControl(method = "cv", number = 5)
metric <- "Accuracy"

set.seed(2020)
tweets.fit <- train(
  form = Class ~ .,
  data = df.tr,
  method = "lda",
  trControl = train_control,
  metric = metric,
  na.action=na.exclude)


pred.te <- predict(tweets.fit, newdata = df.te)
cm5 <- confusionMatrix(data=pred.te, reference = df.te$Class)
draw_confusion_matrix(cm5)
```

Just like the neural network model, the LDA is not good enough (0.68). 

### Political party prediction

Then, we repeat the previous process to predict the political party (y = party).

#### TF + LSA 

```{r include=FALSE}
#we add new features for our model
y <- factor(docvars(tweets.tokens, "party")) #political party is the variable to predict
tweets.dfm <- dfm(tweets.tokens)

nd.vec <- c(2,5,25,50,100, 500, 1000) #because our matrix is well too big, we apply a reduction dimension technique such as the LSA one

acc.vec <- numeric(length(nd.vec))
for (j in 1:length(nd.vec)){
  tweets.lsa <- textmodel_lsa(tweets.dfm, nd=nd.vec[j])
  df <- data.frame(Class=y, X=tweets.lsa$docs)
  df <- cbind(df, 
            logretweet=log10(docvars(tweets.tokens, c("retweet_count"))),
            length = log(sapply(tweets.tokens, length)))
  df.tr <- df[index.tr,]
  df.te <- df[-index.tr,]
  set.seed(2020)
  tweets.fit <- ranger(Class ~ ., #use a random forest
                       data = df.tr)
  pred.te <- predict(tweets.fit, df.te)
  acc.vec[j] <- confusionMatrix(data=pred.te$predictions, reference = df.te$Class)$overall[1]
}

plot(acc.vec ~ nd.vec, type='b',  main = "LSA on a DTM matrix composed by the word frequency", ylab = "Accuracy", xlab = "Number of LSA dimensions") #50 dimensions has the best accuracy

acc.vec
```

The best accuracy (0.8576779) is found with a LSA of 50 dimensions. 


#### TF-IDF + LSA

```{r}
tweets.tfidf <- dfm_tfidf(tweets.dfm)

nd.vec <- c(2,5,25,50,100, 500, 1000) #because our matrix is well too big, we apply a reduction dimension technique such as the LSA one

acc.vec <- numeric(length(nd.vec))
for (j in 1:length(nd.vec)){
  tweets.lsa <- textmodel_lsa(tweets.tfidf, nd=nd.vec[j])
  df <- data.frame(Class=y, X=tweets.lsa$docs)
   df <- cbind(df, 
            logretweet=log10(docvars(tweets.tokens, c("retweet_count"))),
            length = log(sapply(tweets.tokens, length)))
  df.tr <- df[index.tr,]
  df.te <- df[-index.tr,]
  set.seed(2020)
  tweets.fit <- ranger(Class ~ ., #use a random forest
                       data = df.tr)
  pred.te <- predict(tweets.fit, df.te)
  acc.vec[j] <- confusionMatrix(data=pred.te$predictions, reference = df.te$Class)$overall[1]
}

acc.vec
plot(acc.vec ~ nd.vec, type='b', main = "LSA with TF-IDF", ylab = "Accuracy", xlab = "Number of LSA dimensions") #50 dimensions has the best accuracy
```

The random forest trained on a TF-IDF with a LSA of 50 dimensions get an accuracy of 0.8928839 which is better than with TF. Therefore, we will continue with the TF-IDF. 

#### Word embedding + centroids

```{r}
p <- 25 #word embedding dimension

tweets.glove <- GlobalVectors$new(rank = p, x_max = 10) #GloVe method
tweets.we <- tweets.glove$fit_transform(tweets_coo) #central vectors
word_vectors_context <- tweets.glove$components #context vectors
twitter.glove <- tweets.we + t(word_vectors_context) #match central vectors and context vectors

ndoc <- length(tweets.tokens) #number of documents in the corpus 
centers <- matrix(nrow = ndoc, ncol = p) #create a matrix for the centroids
for(i in 1:ndoc){ #compute the centroids 
  words_in_i <- twitter.glove[tweets.tokens[[i]],, drop = FALSE]
  centers[i,] <- apply(words_in_i, 2, mean)
}

row.names(centers) <- names(tweets.tokens) #name the columns of the centroid matrix

df <- data.frame(Class=y, X=centers)
df.tr <- df[index.tr,]
df.te <- df[-index.tr,]
set.seed(2020)
tweets.fit <- ranger(Class ~ ., 
                     data = df.tr)
pred.te <- predict(tweets.fit, df.te)
cm6 <- confusionMatrix(data=pred.te$predictions, reference = df.te$Class)
draw_confusion_matrix(cm6)
```

The random forest trained on the centroids do quite a good job (0.848). Therefore, centroids can be combined with other features to improve the model. 


#### Combining centroids with TF-IDF + LSA 

```{r}
tweets.tfidf <- dfm_tfidf(tweets.dfm) #TF-IDF
tweets.lsa <- textmodel_lsa(tweets.tfidf, nd=50) 
df <- data.frame(Class=y, X=tweets.lsa$docs)
df <- cbind(df, 
            logretweet=log10(docvars(tweets.tokens, c("retweet_count"))),
            length = log(sapply(tweets.tokens, length)))
df <- cbind(df, Cent=centers) # add the centroid from word embedding part
df.tr <- df[index.tr,]
df.te <- df[-index.tr,]
set.seed(2020)
tweets.fit <- ranger(Class ~ ., 
                     data = df.tr, 
                     importance = "impurity")
pred.te <- predict(tweets.fit, df.te)
cm7 <- confusionMatrix(data=pred.te$predictions, reference = df.te$Class)
draw_confusion_matrix(cm7)
```

With an accuracy of 0.885, we can conclude that centroids do not improve our TF-IDF + LSA model. 

#### Support vector machine (SVM) with centroids + TF-IDF + LSA 

```{r}
is.na(df) <- sapply(df, is.infinite) #transform infinite values into NA 
df<- drop_na(df) #remove na values
df.tr <- df[index.tr,]
df.te <- df[-index.tr,]

train_control <- trainControl(method = "cv", number = 5)
metric <- "Accuracy"

set.seed(2020)
fit_svm <- train(
  form = Class ~ .,
  data = df.tr,
  trControl = train_control,
  tuneLength = 5,
  method = "svmRadial",
  preProcess = c("center","scale"),
  metric = metric,
  na.action=na.exclude
)

pred.te <- predict(tweets.fit, df.te)
cm8<- confusionMatrix(data=pred.te$predictions, reference = df.te$Class)
draw_confusion_matrix(cm8)
```

SVM is doing a great job with an accuracy of 0.964 In addition, there is a high specificity and a high sensitivity which are quite well balanced.

#### Neural network (NN) with centroids + TF-IDF + LSA 

```{r}
hp_nn <- expand.grid(size = 2:10,
                     decay = seq(0, 0.5, 0.05))

train_control <- trainControl(method = "cv", number = 5)
metric <- "Accuracy"

set.seed(2020)
tweets.fit <- train(
  form = Class ~ .,
  data = df.tr,
  tuneGrid = hp_nn,
  method = "nnet",
  trControl = train_control,
  metric = metric,
  na.action=na.exclude)

pred.te <- predict(tweets.fit, newdata = df.te)
cm9 <- confusionMatrix(data=pred.te, reference = df.te$Class)
draw_confusion_matrix(cm9)
```

Accuracy : 0.893

#### Linear discriminant analysis (LDA) with centroids + TF-IDF + LSA 

```{r}
train_control <- trainControl(method = "cv", number = 5)
metric <- "Accuracy"

set.seed(2020)
tweets.fit <- train(
  form = Class ~ .,
  data = df.tr,
  method = "lda",
  trControl = train_control,
  metric = metric,
  na.action=na.exclude)

pred.te <- predict(tweets.fit, newdata = df.te)
cm10 <- confusionMatrix(data=pred.te, reference = df.te$Class)
draw_confusion_matrix(cm10)
```


Accuracy : 0.87
